import cv2
import gc
import os

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import seaborn as sb

from sklearn.model_selection import train_test_split
from sklearn import metrics

from PIL import Image
from glob import glob
from tensorflow import keras
from keras import layers
from keras import Model

#MODEL WITH MULTI - INPUT (mediapipe location datapoints)

path = "/Users/emilianeve/Documents/Uni - Sem 1 2023/METR4911/Location Data Images - Test"
classes = os.listdir(path)
classes.remove(".DS_Store")


### DATA PREPARATION ###
IMG_SIZE = 256
SPLIT = 0.2
EPOCHS = 10
BATCH_SIZE = 64

X = []
Y = []

df_processed = pd.DataFrame()
imagelist = []

for i, cat in enumerate(classes):
    #images = glob(f'{path}/{cat}/*.jpg')
    keypoint_files = glob(f'{path}/{cat}/*.xlsx')
    print(keypoint_files)

    for file in keypoint_files:
        df = pd.read_excel(file,index_col = 0)
        
        df['Location'] = i 
        
        for index, row in df.iterrows():
            image = df.loc[index, 'File Name']
            filename = path + '/' + cat + "/" + image

            img = cv2.imread(filename)
            imagelist.append(cv2.resize(img, (IMG_SIZE,IMG_SIZE)))
            Y.append(i)
        
        
        df.drop("File Name", axis = 1, inplace=True)

        df_processed = pd.concat([df_processed, df], ignore_index = True)

imagelist = np.asarray(imagelist)
# print(df_processed)

# df_processed.drop(columns = 0, inplace = True)
df_processed.to_numpy()

# Separating data into testing and training
(train_data_X, test_data_X, train_image_X, test_image_X) = train_test_split(df_processed,imagelist, test_size = SPLIT, random_state= 2022)


train_y = train_data_X["Location"] 
test_y = test_data_X["Location"]

train_data_X.pop("Location")
test_data_X.pop("Location")

train_y_image = pd.get_dummies(train_y).values #train y
test_y_image = pd.get_dummies(test_y).values #Y_VAL


### MODEL INITIALIZATION ###
#Keypoint Model (MLP)
normalise = layers.Normalization(name='keypoints_normalise')
normalise.adapt(train_data_X)

data_shape = train_data_X.shape[1]
keypoint_input = layers.Input(shape = data_shape, name = "keypoints")
keypoint_model = normalise(keypoint_input)
keypoint_model = layers.Dense(128, activation='relu')(keypoint_model)

keypoint_output = layers.Dropout(0.3)(keypoint_model)
print(keypoint_output.shape)

#Image Model(CNN)
image_input = layers.Input(shape = (IMG_SIZE,IMG_SIZE, 3), name = "img")
x = layers.Conv2D(filters=32,
                  kernel_size=(5, 5),
                  activation='relu',
                  input_shape=(IMG_SIZE,
                               IMG_SIZE,
                               3),
                  padding='same')(image_input)
x = layers.MaxPooling2D(2,2)(x)
x = layers.Conv2D(filters=64, 
                  kernel_size=(3,3), 
                  activation='relu',
                  padding='same')(x)
x = layers.MaxPooling2D(2,2)(x)
x = layers.Conv2D(filters=128,
                  kernel_size=(3, 3),
                  activation='relu',
                  padding='same')(x)
x = layers.MaxPooling2D(2,2)(x)
x = layers.Flatten()(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dense(128, activation = 'relu')(x)
image_output= layers.Dropout(0.3)(x)

print(image_output.shape)

#Combing the keypoint and image outputs
x = layers.concatenate([image_output,keypoint_output], name = 'concat_img__kpoint')
predictions = layers.Dense(16, activation = 'softmax')(x)

model = keras.Model(inputs = [image_input, keypoint_input], outputs = predictions)

model.compile(optimizer ="adam", loss = "categorical_crossentropy", metrics = ['accuracy'])

### MODEL TRAINING ###
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if logs.get('val_accuracy') > 0.90:
            print('\n Validation accuracy has reached upto \
                      90% so, stopping further training.')
            self.model.stop_training = True
 
 
es = EarlyStopping(patience=3,
                   monitor='val_accuracy',
                   restore_best_weights=True)
 
lr = ReduceLROnPlateau(monitor='val_loss',
                       patience=2,
                       factor=0.5,
                       verbose=1)

history = model.fit([train_image_X,train_data_X],
                    train_y_image,
                    validation_data = ([test_image_X,test_data_X],test_y_image),
                    batch_size = BATCH_SIZE,
                    epochs = EPOCHS,
                    verbose = 1,
                    callbacks = [es, lr, myCallback()])

#SAVE THE MODEL 
saved_model_path = "/Users/emilianeve/Documents/Uni - Sem 1 2023/METR4911/Saved Models/Model - Experiment v3"
model.save(saved_model_path)



###MODEL EVALUATION ###
Y_pred = model.predict([test_image_X,test_data_X])
Y_val = np.argmax(test_y_image,axis= 1)
Y_pred = np.argmax(Y_pred, axis = 1)

print(metrics.confusion_matrix(Y_val, Y_pred))

print(metrics.classification_report(Y_val, Y_pred,
                                    target_names=classes))

acc = np.array(history.history['accuracy'])
val_acc = np.array(history.history['val_accuracy'])
loss = np.array(history.history['loss'])
val_loss = np.array(history.history['val_loss'])

logs = pd.DataFrame(np.array([acc, val_acc, loss, val_loss]))
logs.to_csv(saved_model_path + "/logs.csv")




